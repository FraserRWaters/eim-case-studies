---
title: What difference does it make?
Template: LeafPage
---

**What difference does it make?**

The argument from no difference is characterised in two ways by Glover, whom I quote directly: 

- *'If I don't do it, someone else will.'*
- *'One person makes no difference.'*

I will here focus primarily on the former argument, which is perhaps more of direct relevance to mathematics. For instance, consider the mathematician headhunted by the unscrupulous financial firm, or a number theorist equivocating over whether to publish a paper that could damage the security of an encryption scheme. Both, if they consider the ethical implications at all, might easily take up such an argument if pressed about the adverse impacts of their actions. After all, they may reason, if they do not do it, some other mathematician could easily be employed to do the job, some other academic will prove the result if they do not – and, importantly, one knows nothing of the ethical leanings of these other mathematicians. As such, given that they stand both to extract substantial personal benefits and to minimise ethical fallout, they may as well – indeed, they may be ethically *obliged* – to perform the action in question.


*When does the argument work?*

This argument can certainly hold together, though in some relatively restricted, fairly ideal, cases. Suppose, for instance, that you are invited to perform an ethically questionable act for a firm. You know that only you and a few other people can perform the task, and that the other people have terrible opinions and would certainly perform the task in some dreadful manner that you strongly desire to avoid, and which, if you were doing the task, you would not let take place; you also know that the firm is completely committed to the task occurring, and will certainly employ someone to do it. This is, I think, probably the closest the argument can come to giving a definitive ethical answer (though of course even in this case, opinions in practice would be far from unanimous).

Yet relax any one of these factors and the whole decision is irretrievably muddied. If the other mathematicians have similar opinions to you, your refusal would usually lead to a similar outcome anyway; there also rises the possibility of co-ordination between the mathematicians to all refuse, leading to the action never occurring, clearly a better ethical outcome. If there were no way to temper the impact of the action, you doing it rather than anyone else becomes a moot point, regardless of their motivations. If the firm is not terribly committed to the task, sufficiently many mathematicians refusing could lead to them abandoning the idea, fearing bad PR. Even in a relatively simple real-life situation, analysis of these factors will be confounded by the unknowable nature of much of the information, as well as the sheer quantity of people involved, and the difficulty of precisely predicting the impacts of any given action.

*Ethics in retrospect*

Can it make sense to define ethics in retrospect? The problem arises, for instance, in the case of proving and publishing some theorem, say for the sake of argument in an obscure corner of number theory. At the time of proof, one has no idea whether it lays the foundation for sure ruin in the path of cryptography, or whether it will remain recondite and ineffectual. The latter, at least within the lifetime of the mathematician, will usually appear to be more likely.

There is a tendency, however, to look at ethics in hindsight to an excessive degree. Papers which led to the Logjam attack, say, are unethical; otherwise similar papers with less applicable results are perfectly fine. This is despite the often obvious fact that usually neither of the authors had any idea what their work led onto. Is some kind of probabilistic apportionment of some metric of 'ethicalness' a workable idea? Does it make any logical sense? I doubt we can usefully quantify 'ethicalness' anyway, so it would need to one carefully, probably qualitatively.

Of course, sometimes, when working in industry, the mathematician will come up with a result whose application is quite obvious in context; they must then weigh up the twin factors of adverse effects of its use, and the vague sense of 'duty to mathematics' which makes many feel obliged to publish such a result.

*Ethical degradation*

Another problem with the argument as stated is that, by and large, people are poor judges of their own moral capabilities. One might easily think that, if one were to start working at a massive bank, one would not be corrupted, one would refuse to perform ethically-questionable tasks, and so forth; yet, a little reflection raises a problem. In considering people at the start of, say, a banking career in line with our usual experience of humanity, it would really be remarkable if most people did not share this line of thinking; in other words, how many people are actually beginning their careers with the express intention of acting immorally? Some, certainly – man is a curious creature – but by no means all. And yet the banks continue in their old machinations, or whatever. 

Paskins (1975) presents some examples of how situations arise in which ethically conscious people surrounded by what they personally consider unethical can easily become complicit, while retaining their ethical beliefs throughout, which I discuss below. 

**Bad luck.** In this instance, the mathematician ends up working at the bank, only to discover that they are in no position to improve the ethical situation, lacking sufficient power. At this point, leaving the firm would be economically ruinous (and an abrogation of ethical responsibility), and so they stay on.

(This may sometimes be explained away by reference to the fact that early jobs within a firm will often lack much individual power in practice, and that roles such as these must be worked through. Note, however, that in order to reach these positions of 'real power', one typically must start from an entry-level job and work enthusiastically for the firm. By the time one eventually reaches the top of the pyramid, one has spent long enough in the firm to both have performed a lot of ethically questionable actions and to have become inured to such actions in a sort of Stockholm syndrome.)

**Exhaustion of the will.** The mathematician, who often was never educated in morality, never held on to his views that tight in the first place. After the gradual destruction of his ethical independence as described above, he finds himself, after some years, cognisant that he has betrayed his moral principles and yet lacking the will or energy to act upon this fact, stifled by the atmosphere of the firm.

**Loyalty.** The mathematician, plied with favourable working conditions, drinks, new friends and much else, soon feels a sense of obligation toward the firm he once despised. Soon finding himself doing actions handed down to him without really thinking, he eventually realises how far he has strayed from his previous beliefs. Again, however, his sense of duty to the firm temporarily blinds him of this fact any time he is given a problematic task. 

There are, no doubt, people with the moral fortitude to avoid these 'traps', but these are rare souls; more pressingly, no-one will truly know whether he is one of these people until he is in the situation, and by then the road back is closed off. Paskins then argues that on reaching any of these traps, one will be unable to justify one's actions to oneself, reaching an ethical impasse that can only be avoided by never entering the firm at all.
